[
  {
    "name": "BERT-Base",
    "year": 2018,
    "organization": "Google",
    "parameters_billions": 0.11,
    "architecture_type": "Transformer Encoder",
    "training_tokens_billions": 3.3,
    "training_compute_flops": 9.2e18,
    "training_days": 4,
    "context_window": 512,
    "max_output_tokens": 512,
    "capability_score_reasoning": 35,
    "capability_score_coding": 20,
    "capability_score_math": 25,
    "capability_score_knowledge": 60,
    "capability_score_multilingual": 40,
    "cost_per_1m_input_tokens": 0,
    "cost_per_1m_output_tokens": 0,
    "num_layers": 12,
    "hidden_size": 768,
    "attention_heads": 12,
    "release_date": "2018-10",
    "model_type": "text",
    "open_source": true,
    "notes": "Bidirectional encoder - revolutionized NLP"
  },
  {
    "name": "GPT-2",
    "year": 2019,
    "organization": "OpenAI",
    "parameters_billions": 1.5,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 40,
    "training_compute_flops": 1.5e20,
    "training_days": 7,
    "context_window": 1024,
    "max_output_tokens": 1024,
    "capability_score_reasoning": 40,
    "capability_score_coding": 25,
    "capability_score_math": 20,
    "capability_score_knowledge": 55,
    "capability_score_multilingual": 30,
    "cost_per_1m_input_tokens": 0,
    "cost_per_1m_output_tokens": 0,
    "num_layers": 48,
    "hidden_size": 1600,
    "attention_heads": 25,
    "release_date": "2019-02",
    "model_type": "text",
    "open_source": true,
    "notes": "Demonstrated impressive zero-shot capabilities"
  },
  {
    "name": "T5-11B",
    "year": 2019,
    "organization": "Google",
    "parameters_billions": 11,
    "architecture_type": "Transformer Encoder-Decoder",
    "training_tokens_billions": 1000,
    "training_compute_flops": 2.7e21,
    "training_days": 30,
    "context_window": 512,
    "max_output_tokens": 512,
    "capability_score_reasoning": 50,
    "capability_score_coding": 30,
    "capability_score_math": 35,
    "capability_score_knowledge": 65,
    "capability_score_multilingual": 45,
    "cost_per_1m_input_tokens": 0,
    "cost_per_1m_output_tokens": 0,
    "num_layers": 24,
    "hidden_size": 1024,
    "attention_heads": 128,
    "release_date": "2019-10",
    "model_type": "text",
    "open_source": true,
    "notes": "Text-to-text transfer transformer"
  },
  {
    "name": "GPT-3 (Davinci)",
    "year": 2020,
    "organization": "OpenAI",
    "parameters_billions": 175,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 300,
    "training_compute_flops": 3.14e23,
    "training_days": 34,
    "context_window": 2048,
    "max_output_tokens": 2048,
    "capability_score_reasoning": 65,
    "capability_score_coding": 50,
    "capability_score_math": 40,
    "capability_score_knowledge": 80,
    "capability_score_multilingual": 60,
    "cost_per_1m_input_tokens": 2.0,
    "cost_per_1m_output_tokens": 2.0,
    "num_layers": 96,
    "hidden_size": 12288,
    "attention_heads": 96,
    "release_date": "2020-06",
    "model_type": "text",
    "open_source": false,
    "notes": "First model to demonstrate strong few-shot learning"
  },
  {
    "name": "GPT-3.5 (text-davinci-003)",
    "year": 2022,
    "organization": "OpenAI",
    "parameters_billions": 175,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 300,
    "training_compute_flops": 3.14e23,
    "training_days": 34,
    "context_window": 4096,
    "max_output_tokens": 4096,
    "capability_score_reasoning": 72,
    "capability_score_coding": 60,
    "capability_score_math": 55,
    "capability_score_knowledge": 82,
    "capability_score_multilingual": 68,
    "cost_per_1m_input_tokens": 1.5,
    "cost_per_1m_output_tokens": 2.0,
    "num_layers": 96,
    "hidden_size": 12288,
    "attention_heads": 96,
    "release_date": "2022-11",
    "model_type": "text",
    "open_source": false,
    "notes": "RLHF-tuned version of GPT-3"
  },
  {
    "name": "LLaMA-65B",
    "year": 2023,
    "organization": "Meta",
    "parameters_billions": 65,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 1400,
    "training_compute_flops": 6.3e23,
    "training_days": 21,
    "context_window": 2048,
    "max_output_tokens": 2048,
    "capability_score_reasoning": 68,
    "capability_score_coding": 55,
    "capability_score_math": 50,
    "capability_score_knowledge": 75,
    "capability_score_multilingual": 62,
    "cost_per_1m_input_tokens": 0,
    "cost_per_1m_output_tokens": 0,
    "num_layers": 80,
    "hidden_size": 8192,
    "attention_heads": 64,
    "release_date": "2023-02",
    "model_type": "text",
    "open_source": true,
    "notes": "Trained on 1.4T tokens, highly efficient"
  },
  {
    "name": "GPT-4",
    "year": 2023,
    "organization": "OpenAI",
    "parameters_billions": 1760,
    "architecture_type": "Mixture of Experts",
    "training_tokens_billions": 13000,
    "training_compute_flops": 2.15e25,
    "training_days": 90,
    "context_window": 8192,
    "max_output_tokens": 4096,
    "capability_score_reasoning": 86,
    "capability_score_coding": 82,
    "capability_score_math": 78,
    "capability_score_knowledge": 92,
    "capability_score_multilingual": 85,
    "cost_per_1m_input_tokens": 30.0,
    "cost_per_1m_output_tokens": 60.0,
    "num_layers": 120,
    "hidden_size": 18432,
    "attention_heads": 128,
    "release_date": "2023-03",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "First major multimodal LLM, accepts images"
  },
  {
    "name": "Claude 2.0",
    "year": 2023,
    "organization": "Anthropic",
    "parameters_billions": 137,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 2000,
    "training_compute_flops": 1.8e24,
    "training_days": 60,
    "context_window": 100000,
    "max_output_tokens": 4096,
    "capability_score_reasoning": 82,
    "capability_score_coding": 78,
    "capability_score_math": 75,
    "capability_score_knowledge": 88,
    "capability_score_multilingual": 80,
    "cost_per_1m_input_tokens": 8.0,
    "cost_per_1m_output_tokens": 24.0,
    "num_layers": 64,
    "hidden_size": 12288,
    "attention_heads": 96,
    "release_date": "2023-07",
    "model_type": "text",
    "open_source": false,
    "notes": "100K context window, strong safety features"
  },
  {
    "name": "LLaMA 2 70B",
    "year": 2023,
    "organization": "Meta",
    "parameters_billions": 70,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 2000,
    "training_compute_flops": 1.7e24,
    "training_days": 24,
    "context_window": 4096,
    "max_output_tokens": 4096,
    "capability_score_reasoning": 73,
    "capability_score_coding": 65,
    "capability_score_math": 60,
    "capability_score_knowledge": 78,
    "capability_score_multilingual": 70,
    "cost_per_1m_input_tokens": 0,
    "cost_per_1m_output_tokens": 0,
    "num_layers": 80,
    "hidden_size": 8192,
    "attention_heads": 64,
    "release_date": "2023-07",
    "model_type": "text",
    "open_source": true,
    "notes": "Open source, commercially usable"
  },
  {
    "name": "Mistral 7B",
    "year": 2023,
    "organization": "Mistral AI",
    "parameters_billions": 7.3,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 1000,
    "training_compute_flops": 5.6e22,
    "training_days": 14,
    "context_window": 8192,
    "max_output_tokens": 8192,
    "capability_score_reasoning": 70,
    "capability_score_coding": 62,
    "capability_score_math": 58,
    "capability_score_knowledge": 72,
    "capability_score_multilingual": 68,
    "cost_per_1m_input_tokens": 0,
    "cost_per_1m_output_tokens": 0,
    "num_layers": 32,
    "hidden_size": 4096,
    "attention_heads": 32,
    "release_date": "2023-09",
    "model_type": "text",
    "open_source": true,
    "notes": "Sliding window attention, highly efficient"
  },
  {
    "name": "GPT-4 Turbo",
    "year": 2023,
    "organization": "OpenAI",
    "parameters_billions": 1760,
    "architecture_type": "Mixture of Experts",
    "training_tokens_billions": 13000,
    "training_compute_flops": 2.15e25,
    "training_days": 90,
    "context_window": 128000,
    "max_output_tokens": 4096,
    "capability_score_reasoning": 88,
    "capability_score_coding": 85,
    "capability_score_math": 82,
    "capability_score_knowledge": 93,
    "capability_score_multilingual": 88,
    "cost_per_1m_input_tokens": 10.0,
    "cost_per_1m_output_tokens": 30.0,
    "release_date": "2023-11",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "128K context, updated knowledge cutoff"
  },
  {
    "name": "Gemini Pro",
    "year": 2023,
    "organization": "Google",
    "parameters_billions": 175,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 4000,
    "training_compute_flops": 5.8e24,
    "training_days": 45,
    "context_window": 32768,
    "max_output_tokens": 8192,
    "capability_score_reasoning": 84,
    "capability_score_coding": 80,
    "capability_score_math": 77,
    "capability_score_knowledge": 90,
    "capability_score_multilingual": 86,
    "cost_per_1m_input_tokens": 0.5,
    "cost_per_1m_output_tokens": 1.5,
    "num_layers": 96,
    "hidden_size": 12288,
    "attention_heads": 96,
    "release_date": "2023-12",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "Native multimodal training"
  },
  {
    "name": "Claude 3 Opus",
    "year": 2024,
    "organization": "Anthropic",
    "parameters_billions": 175,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 3500,
    "training_compute_flops": 4.8e24,
    "training_days": 75,
    "context_window": 200000,
    "max_output_tokens": 4096,
    "capability_score_reasoning": 90,
    "capability_score_coding": 87,
    "capability_score_math": 85,
    "capability_score_knowledge": 93,
    "capability_score_multilingual": 89,
    "cost_per_1m_input_tokens": 15.0,
    "cost_per_1m_output_tokens": 75.0,
    "num_layers": 96,
    "hidden_size": 14336,
    "attention_heads": 112,
    "release_date": "2024-03",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "200K context, strong vision capabilities"
  },
  {
    "name": "Llama 3 70B",
    "year": 2024,
    "organization": "Meta",
    "parameters_billions": 70,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 15000,
    "training_compute_flops": 8.2e24,
    "training_days": 30,
    "context_window": 8192,
    "max_output_tokens": 8192,
    "capability_score_reasoning": 80,
    "capability_score_coding": 75,
    "capability_score_math": 72,
    "capability_score_knowledge": 85,
    "capability_score_multilingual": 78,
    "cost_per_1m_input_tokens": 0,
    "cost_per_1m_output_tokens": 0,
    "num_layers": 80,
    "hidden_size": 8192,
    "attention_heads": 64,
    "release_date": "2024-04",
    "model_type": "text",
    "open_source": true,
    "notes": "Massive training dataset, 15T tokens"
  },
  {
    "name": "GPT-4o",
    "year": 2024,
    "organization": "OpenAI",
    "parameters_billions": 1760,
    "architecture_type": "Mixture of Experts",
    "training_tokens_billions": 20000,
    "training_compute_flops": 3.2e25,
    "training_days": 100,
    "context_window": 128000,
    "max_output_tokens": 16384,
    "capability_score_reasoning": 92,
    "capability_score_coding": 90,
    "capability_score_math": 88,
    "capability_score_knowledge": 95,
    "capability_score_multilingual": 92,
    "cost_per_1m_input_tokens": 5.0,
    "cost_per_1m_output_tokens": 15.0,
    "num_layers": 120,
    "hidden_size": 18432,
    "attention_heads": 128,
    "release_date": "2024-05",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "Omni model with native audio/video/text"
  },
  {
    "name": "Claude 3.5 Sonnet",
    "year": 2024,
    "organization": "Anthropic",
    "parameters_billions": 175,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 5000,
    "training_compute_flops": 6.8e24,
    "training_days": 80,
    "context_window": 200000,
    "max_output_tokens": 8192,
    "capability_score_reasoning": 93,
    "capability_score_coding": 92,
    "capability_score_math": 89,
    "capability_score_knowledge": 94,
    "capability_score_multilingual": 91,
    "cost_per_1m_input_tokens": 3.0,
    "cost_per_1m_output_tokens": 15.0,
    "num_layers": 96,
    "hidden_size": 14336,
    "attention_heads": 112,
    "release_date": "2024-06",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "Best-in-class coding, extended thinking"
  },
  {
    "name": "Gemini 1.5 Pro",
    "year": 2024,
    "organization": "Google",
    "parameters_billions": 175,
    "architecture_type": "Mixture of Experts",
    "training_tokens_billions": 8000,
    "training_compute_flops": 1.1e25,
    "training_days": 60,
    "context_window": 2000000,
    "max_output_tokens": 8192,
    "capability_score_reasoning": 89,
    "capability_score_coding": 86,
    "capability_score_math": 84,
    "capability_score_knowledge": 92,
    "capability_score_multilingual": 90,
    "cost_per_1m_input_tokens": 3.5,
    "cost_per_1m_output_tokens": 10.5,
    "num_layers": 96,
    "hidden_size": 14336,
    "attention_heads": 112,
    "release_date": "2024-07",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "2M context window - longest available"
  },
  {
    "name": "Llama 3.1 405B",
    "year": 2024,
    "organization": "Meta",
    "parameters_billions": 405,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 15000,
    "training_compute_flops": 4.7e25,
    "training_days": 54,
    "context_window": 128000,
    "max_output_tokens": 128000,
    "capability_score_reasoning": 88,
    "capability_score_coding": 85,
    "capability_score_math": 82,
    "capability_score_knowledge": 90,
    "capability_score_multilingual": 87,
    "cost_per_1m_input_tokens": 0,
    "cost_per_1m_output_tokens": 0,
    "num_layers": 126,
    "hidden_size": 16384,
    "attention_heads": 128,
    "release_date": "2024-07",
    "model_type": "text",
    "open_source": true,
    "notes": "Largest open-source model, 128K context"
  },
  {
    "name": "Claude 3.5 Sonnet v2",
    "year": 2024,
    "organization": "Anthropic",
    "parameters_billions": 175,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 6000,
    "training_compute_flops": 8.2e24,
    "training_days": 85,
    "context_window": 200000,
    "max_output_tokens": 8192,
    "capability_score_reasoning": 95,
    "capability_score_coding": 94,
    "capability_score_math": 91,
    "capability_score_knowledge": 95,
    "capability_score_multilingual": 92,
    "cost_per_1m_input_tokens": 3.0,
    "cost_per_1m_output_tokens": 15.0,
    "num_layers": 96,
    "hidden_size": 14336,
    "attention_heads": 112,
    "release_date": "2024-10",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "Computer use capability, improved agentic coding"
  },
  {
    "name": "o1-preview",
    "year": 2024,
    "organization": "OpenAI",
    "parameters_billions": 1760,
    "architecture_type": "Reasoning-Enhanced MoE",
    "training_tokens_billions": 22000,
    "training_compute_flops": 4.0e25,
    "training_days": 120,
    "context_window": 128000,
    "max_output_tokens": 32768,
    "capability_score_reasoning": 98,
    "capability_score_coding": 95,
    "capability_score_math": 96,
    "capability_score_knowledge": 96,
    "capability_score_multilingual": 93,
    "cost_per_1m_input_tokens": 15.0,
    "cost_per_1m_output_tokens": 60.0,
    "num_layers": 120,
    "hidden_size": 18432,
    "attention_heads": 128,
    "release_date": "2024-09",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "Extended chain-of-thought reasoning capabilities"
  }
]
