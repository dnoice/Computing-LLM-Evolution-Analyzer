[
  {
    "name": "BERT-Base",
    "year": 2018,
    "organization": "Google",
    "parameters_billions": 0.11,
    "architecture_type": "Transformer Encoder",
    "training_tokens_billions": 3.3,
    "training_compute_flops": 9.2e18,
    "training_days": 4,
    "context_window": 512,
    "max_output_tokens": 512,
    "capability_score_reasoning": 35,
    "capability_score_coding": 20,
    "capability_score_math": 25,
    "capability_score_knowledge": 60,
    "capability_score_multilingual": 40,
    "cost_per_1m_input_tokens": 0,
    "cost_per_1m_output_tokens": 0,
    "num_layers": 12,
    "hidden_size": 768,
    "attention_heads": 12,
    "release_date": "2018-10",
    "model_type": "text",
    "open_source": true,
    "notes": "Bidirectional encoder that revolutionized NLP with masked language modeling. Foundation for many downstream tasks."
  },
  {
    "name": "GPT-2",
    "year": 2019,
    "organization": "OpenAI",
    "parameters_billions": 1.5,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 40,
    "training_compute_flops": 1.5e20,
    "training_days": 7,
    "context_window": 1024,
    "max_output_tokens": 1024,
    "capability_score_reasoning": 40,
    "capability_score_coding": 25,
    "capability_score_math": 20,
    "capability_score_knowledge": 55,
    "capability_score_multilingual": 30,
    "cost_per_1m_input_tokens": 0,
    "cost_per_1m_output_tokens": 0,
    "num_layers": 48,
    "hidden_size": 1600,
    "attention_heads": 25,
    "release_date": "2019-02",
    "model_type": "text",
    "open_source": true,
    "notes": "Demonstrated impressive zero-shot capabilities and coherent long-form text generation. Initially controversial release."
  },
  {
    "name": "T5-11B",
    "year": 2019,
    "organization": "Google",
    "parameters_billions": 11,
    "architecture_type": "Transformer Encoder-Decoder",
    "training_tokens_billions": 1000,
    "training_compute_flops": 2.7e21,
    "training_days": 30,
    "context_window": 512,
    "max_output_tokens": 512,
    "capability_score_reasoning": 50,
    "capability_score_coding": 30,
    "capability_score_math": 35,
    "capability_score_knowledge": 65,
    "capability_score_multilingual": 45,
    "cost_per_1m_input_tokens": 0,
    "cost_per_1m_output_tokens": 0,
    "num_layers": 24,
    "hidden_size": 1024,
    "attention_heads": 128,
    "release_date": "2019-10",
    "model_type": "text",
    "open_source": true,
    "notes": "Text-to-text transfer transformer. Unified framework treating all NLP tasks as text-to-text problems."
  },
  {
    "name": "GPT-3 (Davinci)",
    "year": 2020,
    "organization": "OpenAI",
    "parameters_billions": 175,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 300,
    "training_compute_flops": 3.14e23,
    "training_days": 34,
    "context_window": 2048,
    "max_output_tokens": 2048,
    "capability_score_reasoning": 65,
    "capability_score_coding": 50,
    "capability_score_math": 40,
    "capability_score_knowledge": 80,
    "capability_score_multilingual": 60,
    "cost_per_1m_input_tokens": 2.0,
    "cost_per_1m_output_tokens": 2.0,
    "num_layers": 96,
    "hidden_size": 12288,
    "attention_heads": 96,
    "release_date": "2020-06",
    "model_type": "text",
    "open_source": false,
    "notes": "First model to demonstrate strong few-shot learning. Breakthrough in language model capabilities and scale."
  },
  {
    "name": "GPT-3.5 (text-davinci-003)",
    "year": 2022,
    "organization": "OpenAI",
    "parameters_billions": 175,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 300,
    "training_compute_flops": 3.14e23,
    "training_days": 34,
    "context_window": 4096,
    "max_output_tokens": 4096,
    "capability_score_reasoning": 72,
    "capability_score_coding": 60,
    "capability_score_math": 55,
    "capability_score_knowledge": 82,
    "capability_score_multilingual": 68,
    "cost_per_1m_input_tokens": 1.5,
    "cost_per_1m_output_tokens": 2.0,
    "num_layers": 96,
    "hidden_size": 12288,
    "attention_heads": 96,
    "release_date": "2022-11",
    "model_type": "text",
    "open_source": false,
    "notes": "RLHF-tuned version of GPT-3 with improved instruction following. Precursor to ChatGPT."
  },
  {
    "name": "GPT-3.5 Turbo",
    "year": 2022,
    "organization": "OpenAI",
    "parameters_billions": 175,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 300,
    "training_compute_flops": 3.14e23,
    "training_days": 35,
    "context_window": 16385,
    "max_output_tokens": 4096,
    "capability_score_reasoning": 78,
    "capability_score_coding": 76,
    "capability_score_math": 72,
    "capability_score_knowledge": 82,
    "capability_score_multilingual": 75,
    "cost_per_1m_input_tokens": 0.5,
    "cost_per_1m_output_tokens": 1.5,
    "num_layers": 96,
    "hidden_size": 12288,
    "attention_heads": 96,
    "release_date": "2022-11",
    "model_type": "text",
    "open_source": false,
    "notes": "Chat-optimized variant. Powered ChatGPT initially. 10x cheaper than Davinci. Very cost-effective for most applications."
  },
  {
    "name": "LLaMA-65B",
    "year": 2023,
    "organization": "Meta",
    "parameters_billions": 65,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 1400,
    "training_compute_flops": 6.3e23,
    "training_days": 21,
    "context_window": 2048,
    "max_output_tokens": 2048,
    "capability_score_reasoning": 68,
    "capability_score_coding": 55,
    "capability_score_math": 50,
    "capability_score_knowledge": 75,
    "capability_score_multilingual": 62,
    "cost_per_1m_input_tokens": 0,
    "cost_per_1m_output_tokens": 0,
    "num_layers": 80,
    "hidden_size": 8192,
    "attention_heads": 64,
    "release_date": "2023-02",
    "model_type": "text",
    "open_source": true,
    "notes": "Trained on 1.4T tokens. Highly parameter-efficient model. Pioneered era of competitive open-source LLMs."
  },
  {
    "name": "GPT-4",
    "year": 2023,
    "organization": "OpenAI",
    "parameters_billions": 1760,
    "architecture_type": "Mixture of Experts",
    "training_tokens_billions": 13000,
    "training_compute_flops": 2.15e25,
    "training_days": 90,
    "context_window": 8192,
    "max_output_tokens": 4096,
    "capability_score_reasoning": 86,
    "capability_score_coding": 82,
    "capability_score_math": 78,
    "capability_score_knowledge": 92,
    "capability_score_multilingual": 85,
    "cost_per_1m_input_tokens": 30.0,
    "cost_per_1m_output_tokens": 60.0,
    "num_layers": 120,
    "hidden_size": 18432,
    "attention_heads": 128,
    "release_date": "2023-03",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "First major multimodal LLM accepting images. 8K base (or 32K at $60/$120). MoE architecture with ~1.76T total params, 8 experts with 2 active per token. Cost of training exceeded $100M."
  },
  {
    "name": "Claude 2.0",
    "year": 2023,
    "organization": "Anthropic",
    "parameters_billions": 137,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 2000,
    "training_compute_flops": 1.8e24,
    "training_days": 60,
    "context_window": 100000,
    "max_output_tokens": 4096,
    "capability_score_reasoning": 82,
    "capability_score_coding": 78,
    "capability_score_math": 75,
    "capability_score_knowledge": 88,
    "capability_score_multilingual": 80,
    "cost_per_1m_input_tokens": 8.0,
    "cost_per_1m_output_tokens": 24.0,
    "num_layers": 64,
    "hidden_size": 12288,
    "attention_heads": 96,
    "release_date": "2023-07",
    "model_type": "text",
    "open_source": false,
    "notes": "100K context window - breakthrough at the time. Strong Constitutional AI safety features and alignment."
  },
  {
    "name": "LLaMA 2 70B",
    "year": 2023,
    "organization": "Meta",
    "parameters_billions": 70,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 2000,
    "training_compute_flops": 1.7e24,
    "training_days": 24,
    "context_window": 4096,
    "max_output_tokens": 4096,
    "capability_score_reasoning": 73,
    "capability_score_coding": 65,
    "capability_score_math": 60,
    "capability_score_knowledge": 78,
    "capability_score_multilingual": 70,
    "cost_per_1m_input_tokens": 0,
    "cost_per_1m_output_tokens": 0,
    "num_layers": 80,
    "hidden_size": 8192,
    "attention_heads": 64,
    "release_date": "2023-07",
    "model_type": "text",
    "open_source": true,
    "notes": "First truly commercially usable open-source LLM with permissive license. Democratized access to capable models."
  },
  {
    "name": "Mistral 7B",
    "year": 2023,
    "organization": "Mistral AI",
    "parameters_billions": 7.3,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 1000,
    "training_compute_flops": 5.6e22,
    "training_days": 14,
    "context_window": 8192,
    "max_output_tokens": 8192,
    "capability_score_reasoning": 70,
    "capability_score_coding": 62,
    "capability_score_math": 58,
    "capability_score_knowledge": 72,
    "capability_score_multilingual": 68,
    "cost_per_1m_input_tokens": 0,
    "cost_per_1m_output_tokens": 0,
    "num_layers": 32,
    "hidden_size": 4096,
    "attention_heads": 32,
    "release_date": "2023-09",
    "model_type": "text",
    "open_source": true,
    "notes": "Sliding window attention mechanism. Highly efficient small model outperforming larger competitors. Apache 2.0 license."
  },
  {
    "name": "GPT-4 Turbo",
    "year": 2023,
    "organization": "OpenAI",
    "parameters_billions": 1760,
    "architecture_type": "Mixture of Experts",
    "training_tokens_billions": 13000,
    "training_compute_flops": 2.15e25,
    "training_days": 90,
    "context_window": 128000,
    "max_output_tokens": 4096,
    "capability_score_reasoning": 88,
    "capability_score_coding": 85,
    "capability_score_math": 82,
    "capability_score_knowledge": 93,
    "capability_score_multilingual": 88,
    "cost_per_1m_input_tokens": 10.0,
    "cost_per_1m_output_tokens": 30.0,
    "num_layers": 120,
    "hidden_size": 18432,
    "attention_heads": 128,
    "release_date": "2023-11",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "Released Nov 6, 2023. 128K context (300 pages). 3x cheaper input, 2x cheaper output than GPT-4. Knowledge cutoff: April 2023. Improved instruction following and JSON mode."
  },
  {
    "name": "Gemini Pro",
    "year": 2023,
    "organization": "Google",
    "parameters_billions": 175,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 4000,
    "training_compute_flops": 5.8e24,
    "training_days": 45,
    "context_window": 32768,
    "max_output_tokens": 8192,
    "capability_score_reasoning": 84,
    "capability_score_coding": 80,
    "capability_score_math": 77,
    "capability_score_knowledge": 90,
    "capability_score_multilingual": 86,
    "cost_per_1m_input_tokens": 0.5,
    "cost_per_1m_output_tokens": 1.5,
    "num_layers": 96,
    "hidden_size": 12288,
    "attention_heads": 96,
    "release_date": "2023-12",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "Google's first major multimodal model family. Native multimodal training from ground up (not bolted on vision)."
  },
  {
    "name": "Claude 3 Opus",
    "year": 2024,
    "organization": "Anthropic",
    "parameters_billions": 175,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 3500,
    "training_compute_flops": 4.8e24,
    "training_days": 75,
    "context_window": 200000,
    "max_output_tokens": 4096,
    "capability_score_reasoning": 90,
    "capability_score_coding": 87,
    "capability_score_math": 85,
    "capability_score_knowledge": 93,
    "capability_score_multilingual": 89,
    "cost_per_1m_input_tokens": 15.0,
    "cost_per_1m_output_tokens": 75.0,
    "num_layers": 96,
    "hidden_size": 14336,
    "attention_heads": 112,
    "release_date": "2024-03",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "200K context window. Strong vision capabilities. Set new benchmarks for intelligence. Claude 3 family: Haiku, Sonnet, Opus. Parameter count estimated (not officially disclosed)."
  },
  {
    "name": "Llama 3 70B",
    "year": 2024,
    "organization": "Meta",
    "parameters_billions": 70,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 15000,
    "training_compute_flops": 8.2e24,
    "training_days": 30,
    "context_window": 8192,
    "max_output_tokens": 8192,
    "capability_score_reasoning": 80,
    "capability_score_coding": 75,
    "capability_score_math": 72,
    "capability_score_knowledge": 85,
    "capability_score_multilingual": 78,
    "cost_per_1m_input_tokens": 0,
    "cost_per_1m_output_tokens": 0,
    "num_layers": 80,
    "hidden_size": 8192,
    "attention_heads": 64,
    "release_date": "2024-04",
    "model_type": "text",
    "open_source": true,
    "notes": "Massive 15T token training dataset - 7x more than Llama 2. Major quality leap in open-source models."
  },
  {
    "name": "Command R+",
    "year": 2024,
    "organization": "Cohere",
    "parameters_billions": 104,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 7000,
    "training_compute_flops": 6.8e24,
    "training_days": 40,
    "context_window": 128000,
    "max_output_tokens": 4096,
    "capability_score_reasoning": 82,
    "capability_score_coding": 79,
    "capability_score_math": 77,
    "capability_score_knowledge": 86,
    "capability_score_multilingual": 84,
    "cost_per_1m_input_tokens": 3.0,
    "cost_per_1m_output_tokens": 15.0,
    "num_layers": 80,
    "hidden_size": 10240,
    "attention_heads": 80,
    "release_date": "2024-04",
    "model_type": "text",
    "open_source": false,
    "notes": "Optimized for RAG (Retrieval Augmented Generation) and enterprise use cases. Strong instruction following and grounded responses."
  },
  {
    "name": "GPT-4o",
    "year": 2024,
    "organization": "OpenAI",
    "parameters_billions": 200,
    "architecture_type": "Multimodal Transformer (Dense)",
    "training_tokens_billions": 10000,
    "training_compute_flops": 8.5e24,
    "training_days": 55,
    "context_window": 128000,
    "max_output_tokens": 16384,
    "capability_score_reasoning": 92,
    "capability_score_coding": 90,
    "capability_score_math": 88,
    "capability_score_knowledge": 95,
    "capability_score_multilingual": 92,
    "cost_per_1m_input_tokens": 2.5,
    "cost_per_1m_output_tokens": 10.0,
    "num_layers": 80,
    "hidden_size": 10240,
    "attention_heads": 80,
    "release_date": "2024-05",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "Released May 13, 2024. 'Omni' model with native text, audio, image, video processing in single model. 2x faster than GPT-4 Turbo. Knowledge cutoff: Oct 2023. Scored 88.7 on MMLU. Real-time voice interaction. Param count estimated (~200B, not 1.76T)."
  },
  {
    "name": "Claude 3.5 Sonnet",
    "year": 2024,
    "organization": "Anthropic",
    "parameters_billions": 175,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 5000,
    "training_compute_flops": 6.8e24,
    "training_days": 80,
    "context_window": 200000,
    "max_output_tokens": 8192,
    "capability_score_reasoning": 93,
    "capability_score_coding": 92,
    "capability_score_math": 89,
    "capability_score_knowledge": 94,
    "capability_score_multilingual": 91,
    "cost_per_1m_input_tokens": 3.0,
    "cost_per_1m_output_tokens": 15.0,
    "num_layers": 96,
    "hidden_size": 14336,
    "attention_heads": 112,
    "release_date": "2024-06",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "Released June 20, 2024. Outperformed Claude 3 Opus on many benchmarks while cheaper. Introduced Artifacts feature for interactive content. Knowledge cutoff: April 2024. 2x speed of Claude 3 Opus. Parameter count estimated."
  },
  {
    "name": "Gemini 1.5 Pro",
    "year": 2024,
    "organization": "Google DeepMind",
    "parameters_billions": 175,
    "architecture_type": "Mixture of Experts",
    "training_tokens_billions": 8000,
    "training_compute_flops": 1.1e25,
    "training_days": 60,
    "context_window": 2000000,
    "max_output_tokens": 8192,
    "capability_score_reasoning": 89,
    "capability_score_coding": 86,
    "capability_score_math": 84,
    "capability_score_knowledge": 92,
    "capability_score_multilingual": 90,
    "cost_per_1m_input_tokens": 3.5,
    "cost_per_1m_output_tokens": 10.5,
    "num_layers": 96,
    "hidden_size": 14336,
    "attention_heads": 112,
    "release_date": "2024-07",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "2M token context window - longest available. Supports text, images, video, audio inputs. Sparse MoE architecture. Can process hours of video or entire codebases."
  },
  {
    "name": "Llama 3.1 405B",
    "year": 2024,
    "organization": "Meta",
    "parameters_billions": 405,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 15000,
    "training_compute_flops": 4.7e25,
    "training_days": 54,
    "context_window": 128000,
    "max_output_tokens": 128000,
    "capability_score_reasoning": 88,
    "capability_score_coding": 85,
    "capability_score_math": 82,
    "capability_score_knowledge": 90,
    "capability_score_multilingual": 87,
    "cost_per_1m_input_tokens": 0,
    "cost_per_1m_output_tokens": 0,
    "num_layers": 126,
    "hidden_size": 16384,
    "attention_heads": 128,
    "release_date": "2024-07",
    "model_type": "text",
    "open_source": true,
    "notes": "Largest open-source model. 128K context. Trained on 16,000 H100 GPUs. Competes with GPT-4 class models. First open model with 128K output capability."
  },
  {
    "name": "Mistral Large 2",
    "year": 2024,
    "organization": "Mistral AI",
    "parameters_billions": 123,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 8000,
    "training_compute_flops": 8.5e24,
    "training_days": 45,
    "context_window": 128000,
    "max_output_tokens": 4096,
    "capability_score_reasoning": 84,
    "capability_score_coding": 86,
    "capability_score_math": 80,
    "capability_score_knowledge": 85,
    "capability_score_multilingual": 88,
    "cost_per_1m_input_tokens": 2.0,
    "cost_per_1m_output_tokens": 6.0,
    "num_layers": 88,
    "hidden_size": 12288,
    "attention_heads": 96,
    "release_date": "2024-07",
    "model_type": "text",
    "open_source": false,
    "notes": "European AI company's flagship model. Exceptionally strong coding and multilingual capabilities, especially European languages."
  },
  {
    "name": "o1-preview",
    "year": 2024,
    "organization": "OpenAI",
    "parameters_billions": 1760,
    "architecture_type": "Reasoning-Enhanced MoE",
    "training_tokens_billions": 22000,
    "training_compute_flops": 4.0e25,
    "training_days": 120,
    "context_window": 128000,
    "max_output_tokens": 32768,
    "capability_score_reasoning": 98,
    "capability_score_coding": 95,
    "capability_score_math": 96,
    "capability_score_knowledge": 96,
    "capability_score_multilingual": 93,
    "cost_per_1m_input_tokens": 15.0,
    "cost_per_1m_output_tokens": 60.0,
    "num_layers": 120,
    "hidden_size": 18432,
    "attention_heads": 128,
    "release_date": "2024-09",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "First model with extended chain-of-thought reasoning. Excels at math, coding, scientific reasoning. 'Thinks' before responding. Performance comparable to PhD-level experts on some benchmarks."
  },
  {
    "name": "Claude 3.5 Sonnet v2",
    "year": 2024,
    "organization": "Anthropic",
    "parameters_billions": 175,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 6000,
    "training_compute_flops": 8.2e24,
    "training_days": 85,
    "context_window": 200000,
    "max_output_tokens": 8192,
    "capability_score_reasoning": 95,
    "capability_score_coding": 94,
    "capability_score_math": 91,
    "capability_score_knowledge": 95,
    "capability_score_multilingual": 92,
    "cost_per_1m_input_tokens": 3.0,
    "cost_per_1m_output_tokens": 15.0,
    "num_layers": 96,
    "hidden_size": 14336,
    "attention_heads": 112,
    "release_date": "2024-10",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "October 2024 update. Added computer use capability - can interact with computer interfaces. Improved agentic coding. Better instruction following."
  },
  {
    "name": "Llama 3.3 70B",
    "year": 2024,
    "organization": "Meta",
    "parameters_billions": 70,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 15000,
    "training_compute_flops": 6.4e24,
    "training_days": 50,
    "context_window": 128000,
    "max_output_tokens": 4096,
    "capability_score_reasoning": 84,
    "capability_score_coding": 82,
    "capability_score_math": 80,
    "capability_score_knowledge": 86,
    "capability_score_multilingual": 83,
    "cost_per_1m_input_tokens": 0,
    "cost_per_1m_output_tokens": 0,
    "num_layers": 80,
    "hidden_size": 8192,
    "attention_heads": 64,
    "release_date": "2024-12",
    "model_type": "text",
    "open_source": true,
    "notes": "Latest Llama 3 iteration. Open-source with permissive license. Excellent performance-to-size ratio. Competes with larger proprietary models."
  },
  {
    "name": "Claude Opus 4.1",
    "year": 2025,
    "organization": "Anthropic",
    "parameters_billions": 750,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 20000,
    "training_compute_flops": 3.0e25,
    "training_days": 95,
    "context_window": 200000,
    "max_output_tokens": 8192,
    "capability_score_reasoning": 97,
    "capability_score_coding": 95,
    "capability_score_math": 94,
    "capability_score_knowledge": 96,
    "capability_score_multilingual": 92,
    "cost_per_1m_input_tokens": 15.0,
    "cost_per_1m_output_tokens": 75.0,
    "num_layers": 108,
    "hidden_size": 20480,
    "attention_heads": 160,
    "release_date": "2025-01",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "Part of Claude 4 family. Most capable model for complex reasoning, analysis, long-form generation. Enhanced agentic capabilities and real-world coding. Parameter count estimated."
  },
  {
    "name": "Claude Sonnet 4.5",
    "year": 2025,
    "organization": "Anthropic",
    "parameters_billions": 650,
    "architecture_type": "Transformer Decoder",
    "training_tokens_billions": 18000,
    "training_compute_flops": 2.5e25,
    "training_days": 85,
    "context_window": 200000,
    "max_output_tokens": 64000,
    "capability_score_reasoning": 96,
    "capability_score_coding": 94,
    "capability_score_math": 92,
    "capability_score_knowledge": 95,
    "capability_score_multilingual": 91,
    "cost_per_1m_input_tokens": 3.0,
    "cost_per_1m_output_tokens": 15.0,
    "num_layers": 104,
    "hidden_size": 18432,
    "attention_heads": 144,
    "release_date": "2025-09",
    "model_type": "multimodal",
    "open_source": false,
    "notes": "Released Sept 29, 2025. Smartest Claude model. Enhanced coding, agents, computer use. 77.2% on SWE-bench Verified. Can maintain focus for 30+ hours on complex tasks. Up to 64K output tokens for long code generation. Parameter count estimated."
  }
]
